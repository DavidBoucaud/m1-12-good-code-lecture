{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing Fast Code\n",
    "\n",
    "The first thing to know about writing fast code is that fast code generally comes **after** making code work.\n",
    "\n",
    "One common saying is *\"Premature optimization is the root of all evil\"* because by trying to make code fast before desining it well, you make everything harder. Here a good pattern for development:\n",
    "\n",
    "1. Decide how fast your code should need to be\n",
    "\n",
    "2. Decide on the tools libraries necessary to achieve that\n",
    "\n",
    "3. Make it work\n",
    "\n",
    "4. Make it fast. Do this by improving the code and **benchmarking** it. Start with the \"hot loop\" that gets run the most, then expand to optimizing more code as necessary.\n",
    "\n",
    "5. Make it easy to read\n",
    "\n",
    "The reason we can't simply \"make it work\" $\\rightarrow$ \"make it fast\" is that if your first design uses libraries that can't fundamentally work at the scale you need might make it hard to change to a faster design afterwards.\n",
    "\n",
    "That said, making your code work is the most important. Making code fast is rarely a necessity as much as delivering working code.\n",
    "\n",
    "# Fast code: a recap\n",
    "\n",
    "The main way to write fast code is to have code be packed tightly in memory so movements between memory systems (CPU registers, CPU cache, RAM, disk drives, internet) are minimized. For reference, here is a handy chart of time taken on a normal x86 CPU to fetch data from different places:\n",
    "\n",
    "```\n",
    "Latency Comparison Numbers (~2012)\n",
    "----------------------------------\n",
    "L1 cache reference                           0.5 ns\n",
    "Branch mispredict                            5   ns\n",
    "L2 cache reference                           7   ns                      14x L1 cache\n",
    "Mutex lock/unlock                           25   ns\n",
    "Main memory reference                      100   ns                      20x L2 cache, 200x L1 cache\n",
    "Compress 1K bytes with Zippy             3,000   ns        3 us\n",
    "Send 1K bytes over 1 Gbps network       10,000   ns       10 us\n",
    "Read 4K randomly from SSD*             150,000   ns      150 us          ~1GB/sec SSD\n",
    "Read 1 MB sequentially from memory     250,000   ns      250 us\n",
    "Round trip within same datacenter      500,000   ns      500 us\n",
    "Read 1 MB sequentially from SSD*     1,000,000   ns    1,000 us    1 ms  ~1GB/sec SSD, 4X memory\n",
    "Disk seek                           10,000,000   ns   10,000 us   10 ms  20x datacenter roundtrip\n",
    "Read 1 MB sequentially from disk    20,000,000   ns   20,000 us   20 ms  80x memory, 20X SSD\n",
    "Send packet CA->Netherlands->CA    150,000,000   ns  150,000 us  150 ms\n",
    "```\n",
    "\n",
    "# Numpy and compiled functions\n",
    "\n",
    "As we saw in the numpy lecture, numpy is fast because arrays are packed in memory and the numpy library functions are written in compiled C code, instead of interpreted python code. \n",
    "\n",
    "Python code has to be re-compiled every time you run it. In a loop it'll re-compile on every iteration. \n",
    "\n",
    "The best way to start optimizing code is to rewrite it to use numpy, pandas or scipy functions instead of python code.\n",
    "\n",
    "\n",
    "# Benchmarking\n",
    "\n",
    "The fastest way to start benchmarking your code is to use the `%timeit` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.5 ms ± 296 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "n = 100000\n",
    "\n",
    "%timeit sum([1. / i**2 for i in range(1, n)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210 µs ± 10.1 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "%timeit np.sum(1. / np.arange(1., n) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory Layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.rand(100_000_000).reshape((10_000, 10_000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76.2 ms ± 14.6 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit a.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84.4 ms ± 17.4 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit a.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summing the array row-wise instead of column-wise is 150% faster, because the data is stored row-by-row.\n",
    "\n",
    "So when iterating through columns, we miss the CPU cache more often\n",
    "\n",
    "In the Pandas library, this is reversed (data is stored by column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profiling\n",
    "\n",
    "On top of benchmarking parts of the code, you can **profile** the code, telling us how much time is spent on each line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If line_profiler isn't installed, install it\n",
    "# !pip install line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('simulation.py', 'w') as f:\n",
    "    f.write(\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "def step(*shape):\n",
    "    # Create a random n-vector with +1 or -1 values.\n",
    "    return 2 * (np.random.random_sample(shape)<.5) - 1\n",
    "\n",
    "def simulate(iterations, n=10000):\n",
    "    s = step(iterations, n)\n",
    "    x = np.cumsum(s, axis=0)\n",
    "    bins = np.arange(-30, 30, 1)\n",
    "    y = np.vstack([np.histogram(x[i,:], bins)[0]\n",
    "                   for i in range(iterations)])\n",
    "    return y\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Profile printout saved to text file 'lprof0'. \n"
     ]
    }
   ],
   "source": [
    "from simulation import simulate\n",
    "\n",
    "%lprun -T lprof0 -f simulate simulate(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profiling RAM usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If memory_profiler isn't installed, install it\n",
    "# !pip install memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('memscript.py', 'w') as f:\n",
    "    f.write(\"\"\"\n",
    "def my_func():\n",
    "    a = [1] * 1000000\n",
    "    b = [2] * 9000000\n",
    "    del b\n",
    "    return a\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "*** Profile printout saved to text file mprof0. \n"
     ]
    }
   ],
   "source": [
    "from memscript import my_func\n",
    "%mprun -T mprof0 -f my_func my_func()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
